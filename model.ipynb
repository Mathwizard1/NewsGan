{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained LLM model and tokenizer (for paraphrasing)\n",
    "# Appropriate choice required for proper usage\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"distilbert-base-uncased\"\n",
    "MY_DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# General Parameter\n",
    "ID2LABEL = {0: \"Polite\", 1: \"Impolite\"}\n",
    "LABEL2ID = {\"Polite\": 0, \"Impolite\": 1}\n",
    "\n",
    "# Selected Hyperparameters\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "TRUNCATION = True\n",
    "PADDING = \"max_length\"\n",
    "RETURN_TENSORS = \"pt\"\n",
    "\n",
    "D_TYPE = torch.bfloat16\n",
    "\n",
    "# Save\n",
    "ADAPTER_NAME = \"PolitenessAdapter\"\n",
    "SAVE_DIR = \"model_politeness_finetuned\\\\\"\n",
    "SAVE_tokenizer = True\n",
    "\n",
    "if(not os.path.exists(SAVE_DIR)):\n",
    "    os.mkdir(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(             #AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype= D_TYPE,\n",
    "    max_length = MAX_SEQUENCE_LENGTH,\n",
    "\n",
    "    # Model label map\n",
    "    num_labels= 2,\n",
    "    id2label= ID2LABEL,\n",
    "    label2id= LABEL2ID\n",
    "    )\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    max_length = MAX_SEQUENCE_LENGTH,\n",
    "    truncation=TRUNCATION,\n",
    "    padding= PADDING,\n",
    "    padding_side= \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token: None, ID: None\n",
      "EOS token: None, ID: None\n",
      "CLS token: [CLS], ID: 101\n",
      "SEP token: [SEP], ID: 102\n",
      "PAD token: [PAD], ID: 0\n",
      "UNK token: [UNK], ID: 100\n",
      "MASK token: [MASK], ID: 103\n"
     ]
    }
   ],
   "source": [
    "# Access special tokens and their IDs\n",
    "print(f\"BOS token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"CLS token: {tokenizer.cls_token}, ID: {tokenizer.cls_token_id}\")\n",
    "print(f\"SEP token: {tokenizer.sep_token}, ID: {tokenizer.sep_token_id}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")\n",
    "print(f\"UNK token: {tokenizer.unk_token}, ID: {tokenizer.unk_token_id}\")\n",
    "print(f\"MASK token: {tokenizer.mask_token}, ID: {tokenizer.mask_token_id}\")\n",
    "\n",
    "# Set the end of string tokens\n",
    "if(tokenizer.pad_token is None):\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 133.91 MB\n",
      "DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory footprint: {model.get_memory_footprint() / 1e6:.2f} MB\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert.embeddings.word_embeddings.weight requires grad = True\n",
      "distilbert.embeddings.position_embeddings.weight requires grad = True\n",
      "distilbert.embeddings.LayerNorm.weight requires grad = True\n",
      "distilbert.embeddings.LayerNorm.bias requires grad = True\n",
      "distilbert.transformer.layer.0.attention.q_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.0.attention.q_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.0.attention.k_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.0.attention.k_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.0.attention.v_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.0.attention.v_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.0.attention.out_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.0.attention.out_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.0.sa_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.0.sa_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.0.ffn.lin1.weight requires grad = True\n",
      "distilbert.transformer.layer.0.ffn.lin1.bias requires grad = True\n",
      "distilbert.transformer.layer.0.ffn.lin2.weight requires grad = True\n",
      "distilbert.transformer.layer.0.ffn.lin2.bias requires grad = True\n",
      "distilbert.transformer.layer.0.output_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.0.output_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.1.attention.q_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.1.attention.q_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.1.attention.k_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.1.attention.k_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.1.attention.v_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.1.attention.v_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.1.attention.out_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.1.attention.out_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.1.sa_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.1.sa_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.1.ffn.lin1.weight requires grad = True\n",
      "distilbert.transformer.layer.1.ffn.lin1.bias requires grad = True\n",
      "distilbert.transformer.layer.1.ffn.lin2.weight requires grad = True\n",
      "distilbert.transformer.layer.1.ffn.lin2.bias requires grad = True\n",
      "distilbert.transformer.layer.1.output_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.1.output_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.2.attention.q_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.2.attention.q_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.2.attention.k_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.2.attention.k_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.2.attention.v_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.2.attention.v_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.2.attention.out_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.2.attention.out_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.2.sa_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.2.sa_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.2.ffn.lin1.weight requires grad = True\n",
      "distilbert.transformer.layer.2.ffn.lin1.bias requires grad = True\n",
      "distilbert.transformer.layer.2.ffn.lin2.weight requires grad = True\n",
      "distilbert.transformer.layer.2.ffn.lin2.bias requires grad = True\n",
      "distilbert.transformer.layer.2.output_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.2.output_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.3.attention.q_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.3.attention.q_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.3.attention.k_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.3.attention.k_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.3.attention.v_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.3.attention.v_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.3.attention.out_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.3.attention.out_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.3.sa_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.3.sa_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.3.ffn.lin1.weight requires grad = True\n",
      "distilbert.transformer.layer.3.ffn.lin1.bias requires grad = True\n",
      "distilbert.transformer.layer.3.ffn.lin2.weight requires grad = True\n",
      "distilbert.transformer.layer.3.ffn.lin2.bias requires grad = True\n",
      "distilbert.transformer.layer.3.output_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.3.output_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.4.attention.q_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.4.attention.q_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.4.attention.k_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.4.attention.k_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.4.attention.v_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.4.attention.v_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.4.attention.out_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.4.attention.out_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.4.sa_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.4.sa_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.4.ffn.lin1.weight requires grad = True\n",
      "distilbert.transformer.layer.4.ffn.lin1.bias requires grad = True\n",
      "distilbert.transformer.layer.4.ffn.lin2.weight requires grad = True\n",
      "distilbert.transformer.layer.4.ffn.lin2.bias requires grad = True\n",
      "distilbert.transformer.layer.4.output_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.4.output_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.5.attention.q_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.5.attention.q_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.5.attention.k_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.5.attention.k_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.5.attention.v_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.5.attention.v_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.5.attention.out_lin.weight requires grad = True\n",
      "distilbert.transformer.layer.5.attention.out_lin.bias requires grad = True\n",
      "distilbert.transformer.layer.5.sa_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.5.sa_layer_norm.bias requires grad = True\n",
      "distilbert.transformer.layer.5.ffn.lin1.weight requires grad = True\n",
      "distilbert.transformer.layer.5.ffn.lin1.bias requires grad = True\n",
      "distilbert.transformer.layer.5.ffn.lin2.weight requires grad = True\n",
      "distilbert.transformer.layer.5.ffn.lin2.bias requires grad = True\n",
      "distilbert.transformer.layer.5.output_layer_norm.weight requires grad = True\n",
      "distilbert.transformer.layer.5.output_layer_norm.bias requires grad = True\n",
      "pre_classifier.weight requires grad = True\n",
      "pre_classifier.bias requires grad = True\n",
      "classifier.weight requires grad = True\n",
      "classifier.bias requires grad = True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # param.requires_grad = False\n",
    "    print(f'{name} requires grad = {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 147,456 || all params: 67,102,466 || trainable%: 0.2197\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\", \n",
    "\n",
    "    r=8,                # The low-rank dimension\n",
    "    lora_alpha=32,      # Scaling factor for LoRA layers\n",
    "    lora_dropout=0.05,  # Dropout probability\n",
    "\n",
    "\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # Target layers\n",
    "    bias=\"none\"                         # Don't apply bias\n",
    ")\n",
    "\n",
    "# set_lora\n",
    "model = get_peft_model(model, lora_config, adapter_name= ADAPTER_NAME)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for base_model.model.distilbert.embeddings.word_embeddings.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.embeddings.position_embeddings.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.embeddings.LayerNorm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.embeddings.LayerNorm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.q_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.q_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.q_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.k_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.k_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.v_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.v_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.v_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.out_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.attention.out_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.sa_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.sa_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.ffn.lin1.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.ffn.lin1.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.ffn.lin2.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.ffn.lin2.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.output_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.0.output_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.q_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.q_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.q_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.k_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.k_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.v_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.v_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.v_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.out_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.attention.out_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.sa_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.sa_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.ffn.lin1.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.ffn.lin1.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.ffn.lin2.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.ffn.lin2.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.output_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.1.output_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.q_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.q_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.q_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.k_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.k_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.v_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.v_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.v_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.out_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.attention.out_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.sa_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.sa_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.ffn.lin1.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.ffn.lin1.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.ffn.lin2.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.ffn.lin2.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.output_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.2.output_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.q_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.q_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.q_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.k_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.k_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.v_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.v_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.v_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.out_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.attention.out_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.sa_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.sa_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.ffn.lin1.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.ffn.lin1.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.ffn.lin2.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.ffn.lin2.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.output_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.3.output_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.q_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.q_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.q_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.k_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.k_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.v_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.v_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.v_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.out_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.attention.out_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.sa_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.sa_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.ffn.lin1.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.ffn.lin1.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.ffn.lin2.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.ffn.lin2.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.output_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.4.output_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.q_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.q_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.q_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.k_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.k_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.v_lin.base_layer.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.v_lin.base_layer.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_A.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.v_lin.lora_B.PolitenessAdapter.weight exist. and requires grad = True\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.out_lin.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.attention.out_lin.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.sa_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.sa_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.ffn.lin1.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.ffn.lin1.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.ffn.lin2.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.ffn.lin2.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.output_layer_norm.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.distilbert.transformer.layer.5.output_layer_norm.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.pre_classifier.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.pre_classifier.bias exist. and requires grad = False\n",
      "Gradients for base_model.model.classifier.weight exist. and requires grad = False\n",
      "Gradients for base_model.model.classifier.bias exist. and requires grad = False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad is not None:\n",
    "        print(f\"Gradients for {name} exist. and requires grad = {param.requires_grad}\")\n",
    "    else:\n",
    "        print(f\"No gradients for {name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Training Parameters\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 1  # Increased for stability\n",
    "NUMBER_OF_EPOCHS = 2\n",
    "\n",
    "GRAD_CLIP = 1.0  # Added for stability\n",
    "\n",
    "\n",
    "NUM_ROWS = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 2)\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "\n",
    "# dataset class for the News\n",
    "class SentenceDataset():\n",
    "    def __init__(self, data_frame_addrs, cols_args= None, batch_size= 1, num_rows = None):\n",
    "        self.data_frame = pd.read_csv(data_frame_addrs)\n",
    "        if(num_rows is not None and num_rows > 0):\n",
    "            self.data_frame = pd.read_csv(data_frame_addrs, nrows= num_rows)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Total samples\n",
    "        print(self.data_frame.shape) \n",
    "        self.length = self.data_frame.shape[0]\n",
    "        self.col_args = self.data_frame.columns\n",
    "\n",
    "        if(cols_args is not None):\n",
    "            self.col_args = cols_args\n",
    "\n",
    "    def print_data(self):\n",
    "        print(self.data_frame)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if(idx + self.batch_size <= self.length):\n",
    "            data = self.data_frame.iloc[idx: idx + self.batch_size]\n",
    "        elif(idx < self.length):\n",
    "            data = pd.concat([self.data_frame.iloc[idx: self.length], self.data_frame.iloc[0: idx + self.batch_size - self.length]])\n",
    "        else:\n",
    "            raise IndexError\n",
    "        # print(type(data))\n",
    "        \n",
    "        # Returns col names per col argument name\n",
    "        return tuple([data[col] for col in self.col_args])\n",
    "            \n",
    "####################################\n",
    "sentence_loader = SentenceDataset('en_train.csv', batch_size= BATCH_SIZE, num_rows= NUM_ROWS)\n",
    "# for i, (sentences, scores) in enumerate(sentence_loader):\n",
    "#     print(i, scores.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template(sentence: str):\n",
    "    return f'''  \n",
    "    [CLS]\n",
    "    Classification: 0 for polite and 1 for impolite.\n",
    "    Sentence => {sentence}\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def torch_scores(scores_list, batch_size = 1):\n",
    "    labels = []\n",
    "    for score in scores_list:\n",
    "        # negative Impolite scores\n",
    "        if(score < 0):\n",
    "            labels.append([0.0, 1.0])\n",
    "        # positive scores\n",
    "        else:\n",
    "            labels.append([1.0, 0.0])\n",
    "    batch_scores = torch.Tensor(labels).to(D_TYPE)\n",
    "    return batch_scores\n",
    "\n",
    "torch_scores([0, 1,-1])\n",
    "# [Polite, Impolite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] classification : 0 for polite and 1 for impolite. sentence = > hello, you? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] classification : 0 for polite and 1 for impolite. sentence = > how? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_prompt(sentences, tokenizer):\n",
    "    # Create the prompts\n",
    "    prompt_inputs = [prompt_template(sentence) for sentence in sentences]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompt_inputs,\n",
    "        add_special_tokens= False,\n",
    "\n",
    "        max_length= MAX_SEQUENCE_LENGTH,\n",
    "        truncation= TRUNCATION,\n",
    "        padding= PADDING,                   # Ensure proper padding\n",
    "        return_tensors= RETURN_TENSORS      # Return tensors for PyTorch\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize the prompt\n",
    "tokenized_inputs = tokenize_prompt([\"hello, you?\", \"how?\"], tokenizer)\n",
    "# print(tokenized_inputs)\n",
    "\n",
    "def detokenizer(tokenized_inputs, tokenizer):\n",
    "    for tokens in tokenized_inputs['input_ids']:\n",
    "        # print(tokens.shape)\n",
    "        decoded_text = tokenizer.decode(tokens, skip_special_tokens= False)\n",
    "        print(decoded_text)\n",
    "\n",
    "detokenizer(tokenized_inputs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr= LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] classification : 0 for polite and 1 for impolite. sentence = > @ smjg, thanks. but why did you also remove the categories i added? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([[0., 1.]], dtype=torch.bfloat16)\n",
      "tensor([[ 0.0024, -0.0471]], dtype=torch.bfloat16, grad_fn=<AddmmBackward0>) <class 'torch.Tensor'>\n",
      "tensor([[0., 1.]], dtype=torch.bfloat16) <class 'torch.Tensor'>\n",
      "torch.Size([1, 2]) torch.Size([1, 2])\n",
      "tensor(0.7070, dtype=torch.bfloat16,\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Epoch [1/2], Loss: 0.0\n",
      "[CLS] classification : 0 for polite and 1 for impolite. sentence = > @ smjg, thanks. but why did you also remove the categories i added? [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "tensor([[0., 1.]], dtype=torch.bfloat16)\n",
      "tensor([[-0.0444, -0.0679]], dtype=torch.bfloat16, grad_fn=<AddmmBackward0>) <class 'torch.Tensor'>\n",
      "tensor([[0., 1.]], dtype=torch.bfloat16) <class 'torch.Tensor'>\n",
      "torch.Size([1, 2]) torch.Size([1, 2])\n",
      "tensor(0.6992, dtype=torch.bfloat16,\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n",
      "Epoch [2/2], Loss: 0.0\n",
      "Training Done!\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "\n",
    "# Training\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, num_epochs):\n",
    "    model.train()  # Set model to \"train\" mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for (sentences, scores) in dataloader:\n",
    "\n",
    "            # print(sentences, scores)\n",
    "            tokenized_inputs = tokenize_prompt(sentences.to_list(), tokenizer)\n",
    "            labels = torch_scores(scores.to_list(), BATCH_SIZE)\n",
    "\n",
    "            # print(tokenized_inputs['input_ids'].shape)\n",
    "            # detokenizer(tokenized_inputs, tokenizer)\n",
    "            # print(labels)\n",
    "\n",
    "            # Tokenized input sentence\n",
    "            input_ids = tokenized_inputs['input_ids'].to(MY_DEVICE)            \n",
    "            attention_mask = tokenized_inputs['attention_mask'].to(MY_DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            \n",
    "            # print(outputs, type(outputs))\n",
    "            # print(labels, type(labels))\n",
    "            # print(outputs.shape, labels.shape)\n",
    "\n",
    "            # Loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # print(loss)\n",
    "\n",
    "            # break\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Apply gradient clipping before optimizer step\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print loss at the end of epoch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "        # Save model at the end of each epoch\n",
    "        model.save_pretrained(SAVE_DIR)\n",
    "        # print(f\"Model saved after epoch {epoch+1}\")\n",
    "\n",
    "    print(\"Training Done!\")\n",
    "    if(SAVE_tokenizer):\n",
    "        tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, sentence_loader, num_epochs= NUMBER_OF_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "Loss: 0.69453125\n"
     ]
    }
   ],
   "source": [
    "def test_model(model: AutoModelForCausalLM, output_folder, dataloader, adapter_name = \"default\"):\n",
    "    model.load_adapter(output_folder + adapter_name, adapter_name)\n",
    "\n",
    "    if(SAVE_tokenizer):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            output_folder,\n",
    "            max_length = MAX_SEQUENCE_LENGTH,\n",
    "            truncation=TRUNCATION,\n",
    "            padding= PADDING,\n",
    "            padding_side= \"right\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    running_loss = 0.0\n",
    "    for sentence, score in dataloader:\n",
    "        # print(sentences, scores)\n",
    "        tokenized_inputs = tokenize_prompt(sentence.to_list(), tokenizer)\n",
    "        labels = torch_scores(score.to_list(), BATCH_SIZE)\n",
    "\n",
    "        # Tokenized input sentence\n",
    "        input_ids = tokenized_inputs['input_ids'].to(MY_DEVICE)            \n",
    "        attention_mask = tokenized_inputs['attention_mask'].to(MY_DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        pred = outputs\n",
    "\n",
    "        # loss\n",
    "        loss = criterion(pred, labels)\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "test_loader = SentenceDataset(\"en_test.csv\", num_rows= 5)\n",
    "test_model(model, SAVE_DIR, test_loader, ADAPTER_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
